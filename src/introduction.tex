\section{Introduction}
\label{sec:introduction}

The increase in demand of highly available data, supported by the growing
exposure of Internet services that make this data accessible, shows that there
is an obvious need today to satisfy requirements that were never before a priority.
Now, features like throughput, consistency, and fault-tolerance are necessities,
not optimizations, and are included in the design of any modern distributed data
system from the beginning. One cannot accept delays in database requests of more
than a few hundred milliseconds or downtimes of even a few minutes. The trend is
clear: we need to process more data, quicker, and without interruptions.

\textit{Replication} is a technique which ensures fault-tolerance on one hand,
while providing the means for achieving higher scalability and performance on the
other hand. However, it introduces the problem of maintaining different replicas
of the same logical data consistent with each other. Ideally, we want the data
stores to be always available, strongly consistent, and to operate flawlessly in
the presence of network failures. CAP is a well known
theorem~\cite{Gilbert:2002:BCF:564585.564601} which states that any distributed
computer system cannot provide simultaneous guarantees for the aforementioned
requirements. The majority of the current Internet services prefer availability
and partition tolerance, at the cost of a weaker form of consistency. The choice
has the advantage of lower latencies for client requests and higher scalability,
but achieving consistency between replicas still remains an open issue.

One attractive approach is to provide \textit{eventual
consistency}~\cite{DBLP:journals/queue/Vogels08a,Saito:2005:OR:1057977.1057980},
which allows any replica to apply updates locally, while the operations are
later sent asynchronously to all the others. In this way, all replicas
eventually apply all updates, possibly even in a different order. With this
weaker form of consistency, considered acceptable for some applications, data
remains available when the network is partitioned. The downside is that a
complex background consensus algorithm for reconciling conflicting updates is
generally needed~\cite{Terry:1995:MUC:224056.224070}, which makes current
approaches ad-hoc and error-prone. Amazon's shopping cart constitutes a
well-known example in this sense~\cite{DeCandia:2007:DAH:1294261.1294281}.
Alternatively, several systems execute an update immediately and later discover
that it conflicts with another~\cite{Terry:1995:MUC:224056.224070}. So they
roll-back to resolve the conflict.

\textit{Conflict-free Replicated Data Types} (CRDTs) were designed specifically
to solve this problem by employing a new type of consistency, \textit{strong
eventual consistency}, as defined in Section~\ref{sec:crdts}. Replicas of CRDTs
are proved to converge in a self-stabilising manner without blocking client
operations and without having to deal with consensus, complex conflict
resolution, or roll-backs. However, since this model imposes some mathematical
constraints, not all data structures are suitable for it.

CRDTs consist in ubiquitous basic structures, e.g. counters, shared mutable
variables, or sets, which can be used as building blocks in forming more complex
ones, like maps or graphs. For a practical use-case scenario, consider how an
event tracking mechanism can be implemented in order to prevent attacks on an
Internet service provider. Filters are used to keep track and to limit the
number of events allowed for a given IP address or account, such as login
attempts, password changes, emails sent, and so on. A replicated counter can
store the number of login attempts from one IP address, while a replicated set
the corresponding unique passwords tried. Since this case requires high
throughput for writes of runtime data and low latencies for reads,
synchronization or conflict resolutions are not acceptable. CRDTs are very
attractive, as all updates are persistent and can immediately be applied locally
at the source replica. Consistency is achieved later, during a background
asynchronous phase in which all replicas eventually apply all updates.
Furthermore, the composability nature of CRDTs allows this use case to be easily
extended to a graph-like structure: store relations among various events and
entities, such as account, IP addresses, aliases, and login attempts in order to
better detect malicious behaviour with heuristic algorithms. Another application
of the CRDTs is cooperative editing~\cite{Preguica:2009:CRD:1584339.1584604}.

The focus of this paper is a particular CRDT: the \textit{set} data type. To
achieve a highly scalable replicated set, the following contributions are made:
\begin{itemize}
  \item Because one variant of CRDTs transfer full states between replicas when
  propagating updates, an improvement to the set type is provided which
  transmits only deltas.
  \item Acknowledging the fact that large data structures cannot be efficiently
  stored on just one machine, a partitioning scheme is often desired. Sets of
  elements fit well in this category of structures, being easily partitioned in
  several disjunctive subsets and distributed across a cluster of machines.
  This solves both the problem of data growth by achieving higher scalability
  and the problem of performance bottleneck by sharing the load. Thus, a second
  contribution is an extension to the set specification to support per-replica
  partitioning capability, or \textit{sharding}.
  \item CRDTs usually lead to an increase in database size with each update
  operation. Also, we want to add a feature for limited-lifetime elements:
  discarding elements older than a given time value. We discuss an asynchronous
  garbage collection mechanism which solves both these issues.
\end{itemize}